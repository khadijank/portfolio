{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMA-B 2023 865, Individual Assignment 1\n",
    "\n",
    "Version 2: Updated June 27, 2022.\n",
    "\n",
    "- [Khadija Naeem]\n",
    "- [Student number]\n",
    "- [October 1, 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - ELI5\n",
    "\n",
    "_“If you can't explain it simply, you don't understand it well enough.” – Albert Einstein_\n",
    "\n",
    "Explaining technical concepts to a non-technical audience is an underappreciated skill; one which the MMA-B program aims to give its students; and one that will truly set you apart in the job market. The only way to gain a skill is by practice, so here we go.\n",
    "\n",
    "Answer each question below as though you were talking to a 5 year old (equivalently: a grandma, or a completely non-technical manager, or an Ivey grad). Use your own words. Use analogies where possible. Examples are better than theory. Keep it short, but be complete. Use simple, plain English. Do not use business buzzwords like _actualize, empower, fungible, leverage, or synergize_. Do not use technical buzzwords that most people don’t know like _model, agile, bandwidth, IoT, blockchain, AR, VR, actionable insights_. Inform the audience without going into too much technical detail. Your goal is to truly help them understand, not to give what you feel is a “technically precise” answer and move on (but they still don’t understand!). Don’t be that guy!\n",
    "\n",
    "Please keep each answer to 1000 characters or less.\n",
    "\n",
    "Finally, feel free to use [Markdown syntax](https://www.markdownguide.org/basic-syntax/) to format your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: What is “Big Data” and how is it different than “regular data”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Data is a concept that describes very large and complex datasets and the technology that is used to handle them. Big data differs from regular data primarily because it requires a lot more memory storage and computing power. Big data has many additional considerations that need to be factored for any individual or organization dealing with them: storage and processing. Big data often requires 'data warehouses' or 'data lakes' that can accomodate the storage size. Businesses also need to make decisions based on cost or security on whether it can be stored via cloud or physically, and also how up-do-date data needs to be processed (real-time vs. in scheduled 'batches'). Essentially, if you are unsure whether you are dealing with big data or not, consider the following:\n",
    "\n",
    "What is the volume (scale and size) of the dataset?\n",
    "\n",
    "What is the speed required to process the data for decision making?\n",
    "\n",
    "Does the data exhibit various forms (eg. text, images, videos, unstructured/structured, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: What is Hadoop? Hint: What problems in previous data storage and processing was Hadoop designed to solve? How did Hadoop accomplish that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is a data processing and storing software. It allows for processing various data types. The concept of 'scaling out' refers to using multiple servers of the same size to process data, as opposed to 'scaling up' which refers to moving data to a single, larger server. Scaling out is much more cost effective and better able to deal with processing failures than scaling up. Hadoop accomplishes this by duplicating data on multiple machines, as well as making each task it computes small, thereby allowing it to be re-done easily if necessary. Hadoop also avoids bottlenecks associated with moving data by applying code to each machine that stores the data instead, making it much faster. Hadoop also avoids the complexity of coding data by allowing the user to apply their commands to each block of data stored on each machine instead of executing multiple commands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: How does Big Data and the cloud help Machine Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud computing refers to computing services, including but not limited to the storage, processing and visualization of data sets, being made available online as opposed to on a physical on-premise server. These services are often made available to users on an easy payment plan. The 'cloud' is what's referred to as being 'online'. These technologies often allow its users to to have access to infrastructure that outperforms the machines and servers onsite, at a much lower cost. Big Data and the cloud can makes Machine Learning effective and even cost-effective, because the combination of larger data sets with the effectiveness of the cloud with regards to computing and storage make predictions that can be made at a faster and more accurate rate. This allows businesses, individuals, researchers, etc. to make better future decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: What is NoSQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoSQL is a term that is sometimes (problematically) used to refer to a repository that stores and manages data that is not structured into columns and rows, also known as non-relational databases. The type of data that can be stored includes:\n",
    "\n",
    "1. Key-value: a large lookup table \n",
    "2. Graph: usually data visualized to describe a graphical network\n",
    "3. Document: collection of documents that each have an associated key id \n",
    "4. Columnar: where data is grouped together by columns only (as a note, relational data has both rows and columns, making columnar data technically non-relational)\n",
    "\n",
    "The purpose of non-relational data repositories is to allow for much faster and scalable databases, as things like graphs, documents, etc. have different requirements for their formatting. However, the downside is that while you gain speed and scalability, you may be giving up certain features that you are unable to view with non-relational data types. The reason why NoSQL is a bad nickname for non-relational databases is because the technology used to store non-relational data can in fact also support relational (aka SQL) databases as well. Non-relational simply refers to the way the data is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Name three ways topic modeling could help a bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Improving customer service: a bank can identify commonly recurring themes from customer feedback forms to improve their call centers, branches, etc. For example, the bank can identify issues with the deliver methods of customer service (email, phone, chat), wait times, time it takes to resolve an issue, etc. \n",
    "\n",
    "2. Product development: a bank can perform social media/media monitoring (tweets, articles, etc.) and use topic modelling techniques to see what concerns certain consumers may have and how their products can address them. Eg. whether younger consumers are more interested in robo-advisors or meeting with a financial advisor in person, or what kind of credit card perks matter the most to students. \n",
    "\n",
    "3. Competitive strategy: using topic modelling on news articles, a bank can conduct business intelligence and industry-wide analysis by on trending topics on consumer attitudes, economic and financial thought leadership, and keep an eye out on the publicly reported activities of their competitors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: What is Apache Spark, exactly, and what are its pros and cons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is a program used for large scale data processing for Hadoop and Big Data in general. It is deemed the best tool currently for a number of reasons:\n",
    "\n",
    "Incredinly fast\n",
    "Many features that support SQL-based operations, Machine Learning, and general data processing/streaming/graphing\n",
    "It can support a number of programming languages such as Python, R, Java, SQL, Scala that can be used in the same environment \n",
    "Is very nice and clean looking for users\n",
    "Requires less code\n",
    "Receives contributions from programmers from over 100 organizations, making it a leading class tool \n",
    "Data and models can be scaled to thousands of singular computers \n",
    "\n",
    "As great as Spark is, it does have some limitations:\n",
    "It requires a lot of memory space and computing power\n",
    "Supports Spark libraries slightly better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Sentiment Analysis via the ML-based approach\n",
    "\n",
    "Download the “Product Sentiment” dataset from the course portal: sentiment_train.csv and sentiment_test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2396 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2396 non-null   object\n",
      " 1   Polarity  2396 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 56.2+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2396 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2396 non-null   object\n",
      " 1   Polarity  2396 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 56.2+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#loading data sets\n",
    "df_train = pd.read_csv(r\"C:\\Users\\khadi\\Desktop\\MMA Program\\MMA 865 Big Data\\Individual Assignment\\sentiment_train.csv\")\n",
    "df_train = df_train[df_train[\"Sentence\"].str.contains(\"#NAME?\") == False]\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(r\"C:\\Users\\khadi\\Desktop\\MMA Program\\MMA 865 Big Data\\Individual Assignment\\sentiment_test.csv\")\n",
    "df_test = df_test[df_test[\"Sentence\"].str.contains(\"#NAME?\") == False]\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before duplication removal:  Sentence    2396\n",
      "Polarity    2396\n",
      "dtype: int64\n",
      "After duplication removal:  Sentence    2381\n",
      "Polarity    2381\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate values\n",
    "print(\"Before duplication removal: \", df_train.count())\n",
    "df_train_distinct = df_train.drop_duplicates()\n",
    "print(\"After duplication removal: \", df_train_distinct.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#EDA - 'Polarity' value count\n",
    "df_train_distinct ['Polarity'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'Polarity' column to split x and y train datasets\n",
    "train_y = df_train_distinct['Polarity']\n",
    "train_x = df_train_distinct['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting test data set\n",
    "test_y = df_test['Polarity']\n",
    "test_x = df_test.drop('Polarity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khadi\\AppData\\Local\\Temp\\ipykernel_101264\\3044236741.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_x = train_x.str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "#Removal of characters, numbers and punctuation from text:\n",
    "train_x = train_x.replace('\\d+', '', regex=True)\n",
    "train_x = train_x.str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 2381)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a list of text data from dataframe\n",
    "text = list(train_x)\n",
    "type(text), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing Pipeline\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# determine the parts of speech for a word\n",
    "# def get_wordnet_pos(word):    \n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# df_test[\"Sentence\"] = df_test['Sentence'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    r = re.sub('[^\\w\\s]','', text[i])\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "    r = re.sub('[0-9\\n]', ' ', text[i])\n",
    "    r = re.sub('[\\d+]', ' ', text[i])\n",
    "    r = r.lower()\n",
    "    r = r.split()\n",
    "    r = [word for word in r if word not in stopwords.words('english')]\n",
    "    r = [stemmer.stem(word) for word in r]\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "    r = ' '.join(r)\n",
    "    corpus.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow love place\n",
      "--------\n",
      "crust good\n",
      "--------\n",
      "tasti textur nasti\n",
      "--------\n",
      "stop late may bank holiday rick steve recommend love\n",
      "--------\n",
      "select menu great price\n",
      "--------\n",
      "get angri want damn pho\n",
      "--------\n",
      "honeslti didnt tast fresh\n",
      "--------\n",
      "potato like rubber could tell made ahead time kept warmer\n",
      "--------\n",
      "fri great\n",
      "--------\n",
      "great touch\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for i in corpus[:10]:\n",
    "    print(i)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n",
    "\n",
    "Use your favorite ML algorithm to train a classification model.  Don’t forget everything that we’ve learned in our ML course: hyperparameter tuning, cross validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing classifier that you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2381x3138 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13268 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "doc = tfidf.fit_transform(corpus)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modelling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=20, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=20, random_state=123)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA.fit(doc)\n",
    "LDA.fit(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3138)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abhor</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abound</th>\n",
       "      <th>abovepretti</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absolutel</th>\n",
       "      <th>absolutley</th>\n",
       "      <th>abstrus</th>\n",
       "      <th>ac</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>yucki</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummi</th>\n",
       "      <th>za</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.649101</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430623</td>\n",
       "      <td>0.942196</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.470827</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.878757</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.331355</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.437295</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.465793</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.392224</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.378051</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.359509</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.390179</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.517797</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463683</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050102</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.405043</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.404593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.37712</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.942224</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.402471</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.448375</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>2.355308</td>\n",
       "      <td>0.404786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.005104</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.436499</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.852795</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262909</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.452106</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.537402</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.409463</td>\n",
       "      <td>0.830536</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.699895</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.714773</td>\n",
       "      <td>3.680299</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.622816</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.46871</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.539108</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.471360</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 3138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abhor      abil       abl    abound  abovepretti   absolut  absolutel  \\\n",
       "0   0.050000  0.050000  0.050000  0.649101     0.050000  0.050000   0.050000   \n",
       "1   0.050000  0.050000  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "2   0.050000  0.050000  0.050000  0.050000     0.050000  0.878757   0.050000   \n",
       "3   0.050000  0.050000  0.050000  0.050000     0.050000  0.331355   0.050000   \n",
       "4   0.050000  0.392224  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "5   0.050000  0.050000  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "6   0.050000  0.050000  0.517797  0.050000     0.050000  0.050000   0.050000   \n",
       "7   0.050000  0.050000  0.050102  0.050000     0.050000  0.405043   0.050000   \n",
       "8   0.050000  0.050000  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "9   0.050000  0.050000  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "10  0.050000  0.050000  0.942224  0.050000     0.050000  0.402471   0.050000   \n",
       "11  0.050000  0.050000  0.050000  0.050000     0.050000  1.005104   0.050000   \n",
       "12  0.050000  0.050000  0.050000  0.050000     0.050000  0.436499   0.050000   \n",
       "13  0.050000  0.050000  0.050000  0.050000     0.050000  0.452106   0.050000   \n",
       "14  0.050000  0.537402  0.050000  0.050000     0.050000  0.409463   0.830536   \n",
       "15  0.050000  0.050000  0.050000  0.050000     0.714773  3.680299   0.050000   \n",
       "16  0.622816  0.050000  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "17  0.050000  0.050000  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "18  0.050000  0.050000  0.050000  0.050000     0.050000  0.050000   0.050000   \n",
       "19  0.050000  0.050000  0.539108  0.050000     0.050000  0.050000   0.050000   \n",
       "\n",
       "    absolutley   abstrus        ac  ...     young      your    youth  \\\n",
       "0     0.050000  0.050000  0.050000  ...  0.430623  0.942196  0.05000   \n",
       "1     0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "2     0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "3     0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "4     0.050000  0.378051  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "5     0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "6     0.050000  0.050000  0.050000  ...  0.463683  0.050000  0.05000   \n",
       "7     0.050000  0.050000  0.404593  ...  0.050000  0.050000  0.37712   \n",
       "8     0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "9     0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "10    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "11    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "12    0.852795  0.050000  0.050000  ...  0.262909  0.050000  0.05000   \n",
       "13    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "14    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "15    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "16    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "17    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "18    0.050000  0.050000  0.050000  ...  0.050000  0.050000  0.05000   \n",
       "19    0.050000  0.050000  0.050000  ...  0.050000  0.471360  0.05000   \n",
       "\n",
       "       yucki     yukon       yum     yummi       za      zero   zombiez  \n",
       "0   0.050000  0.050000  0.470827  0.050000  0.05000  0.050000  0.050000  \n",
       "1   0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "2   0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "3   0.437295  0.050000  0.050000  0.465793  0.05000  0.050000  0.050000  \n",
       "4   0.050000  0.050000  0.050000  0.359509  0.05000  0.050000  0.050000  \n",
       "5   0.050000  0.390179  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "6   0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "7   0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "8   0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "9   0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "10  0.050000  0.050000  0.050000  0.448375  0.05000  2.355308  0.404786  \n",
       "11  0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "12  0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "13  0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "14  0.050000  0.050000  0.050000  0.699895  0.05000  0.050000  0.050000  \n",
       "15  0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "16  0.050000  0.050000  0.050000  0.050000  0.46871  0.050000  0.050000  \n",
       "17  0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "18  0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "19  0.050000  0.050000  0.050000  0.050000  0.05000  0.050000  0.050000  \n",
       "\n",
       "[20 rows x 3138 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(LDA.components_, columns = tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 30 WORDS FOR TOPIC #0\n",
      "['abhor', 'fort', 'forgot', 'rise', 'risk', 'ford', 'risotto', 'rita', 'fool', 'rivet', 'foodand', 'fondu', 'fond', 'robert', 'rocket', 'roeg', 'fo', 'fm', 'flush', 'rotat', 'forth', 'florida', 'forti', 'ringer', 'fruit', 'restock', 'frog', 'resum', 'retard', 'frequent']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #1\n",
      "['abhor', 'replacementr', 'flawless', 'replenish', 'flavour', 'flatlin', 'research', 'flair', 'flag', 'fli', 'fix', 'restaraunt', 'firstperson', 'firehous', 'restock', 'fingernail', 'finger', 'resum', 'financi', 'respect', 'retard', 'replaceeasi', 'repair', 'fort', 'relat', 'forgot', 'forgeri', 'releas', 'fool', 'relleno']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #2\n",
      "['abhor', 'financi', 'repeat', 'filmmak', 'replacementr', 'replenish', 'file', 'figur', 'fiancé', 'femal', 'reset', 'fellow', 'fella', 'respect', 'restaraunt', 'fee', 'fear', 'restock', 'repair', 'fav', 'finger', 'firehous', 'refurb', 'floor', 'regrett', 'regularli', 'reheat', 'relat', 'fli', 'flawless']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #3\n",
      "['abhor', 'muffin', 'mozzarella', 'moz', 'movement', 'morn', 'monster', 'monkey', 'momentum', 'mojito', 'mode', 'mmmm', 'mix', 'misplac', 'mislead', 'mishima', 'mirrormask', 'mirag', 'minutesmajor', 'minor', 'miniusb', 'mine', 'milkshak', 'milk', 'militari', 'mile', 'middl', 'microsoft', 'muffl', 'multigrain']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #4\n",
      "['abhor', 'dwrongli', 'earbug', 'remak', 'earphon', 'relleno', 'releas', 'ebay', 'eccleston', 'reheat', 'regularli', 'edinburgh', 'refurb', 'refri', 'eew', 'reduct', 'electron', 'recognit', 'receptiona', 'email', 'receipt', 'recal', 'readi', 'reaction', 'endear', 'endlessli', 'engin', 'ratio', 'rapidli', 'rank']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #5\n",
      "['abhor', 'mile', 'middl', 'microsoft', 'michael', 'mgm', 'meth', 'metal', 'mesquit', 'merit', 'mere', 'melvil', 'mein', 'meh', 'megapixel', 'mega', 'meeveryth', 'medium', 'mediterranean', 'mechan', 'meatloaf', 'meatbal', 'meant', 'meander', 'meagr', 'max', 'militari', 'milk', 'milkshak', 'mine']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #6\n",
      "['abhor', 'mode', 'mmmm', 'mix', 'misplac', 'mislead', 'mishima', 'mirrormask', 'mirag', 'minutesmajor', 'minor', 'miniusb', 'mine', 'milkshak', 'milk', 'militari', 'mile', 'middl', 'michael', 'mgm', 'meth', 'metal', 'mesquit', 'merit', 'mere', 'melvil', 'melt', 'mein', 'mojito', 'momentum']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #7\n",
      "['abhor', 'mein', 'meh', 'megapixel', 'mega', 'meeveryth', 'meet', 'medium', 'mediterranean', 'mechan', 'mebunch', 'meatloaf', 'meatbal', 'meant', 'meagr', 'max', 'matter', 'masterpiec', 'master', 'masculin', 'martini', 'mark', 'marion', 'marin', 'manna', 'mango', 'melvil', 'mere', 'merit', 'mesquit']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #8\n",
      "['abhor', 'mishima', 'mirrormask', 'mirag', 'minutesmajor', 'minor', 'miniusb', 'mine', 'milkshak', 'milk', 'militari', 'mile', 'middl', 'microsoft', 'michael', 'mgm', 'meth', 'metal', 'mesquit', 'merit', 'mere', 'melvil', 'melt', 'mein', 'meh', 'megapixel', 'mega', 'meeveryth', 'mislead', 'meet']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #9\n",
      "['abhor', 'misplac', 'mislead', 'mishima', 'mirrormask', 'mirag', 'minutesmajor', 'minor', 'miniusb', 'mine', 'milkshak', 'milk', 'militari', 'mix', 'mile', 'microsoft', 'michael', 'mgm', 'meth', 'metal', 'mesquit', 'merit', 'melvil', 'mein', 'meh', 'megapixel', 'mega', 'middl', 'meeveryth', 'mmmm']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #10\n",
      "['abhor', 'unrealist', 'kill', 'kieslowski', 'khao', 'unreli', 'keith', 'stereo', 'kabuki', 'juri', 'june', 'steve', 'journey', 'kinda', 'jone', 'unremark', 'john', 'joey', 'jimmi', 'jiggl', 'jewel', 'jerk', 'jenni', 'jeff', 'jami', 'jamaican', 'jalapeno', 'jonah', 'unsatisfi', 'kindl']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #11\n",
      "['abhor', 'repair', 'fiancé', 'femal', 'fellow', 'fella', 'replacementr', 'replenish', 'fee', 'fear', 'research', 'reset', 'fav', 'respect', 'fat', 'restaraunt', 'restock', 'farc', 'file', 'resum', 'fillet', 'remov', 'flawless', 'regularli', 'flavour', 'reheat', 'relat', 'flatlin', 'releas', 'flair']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #12\n",
      "['abhor', 'ford', 'research', 'fool', 'reset', 'foodand', 'fondu', 'fond', 'respect', 'restaraunt', 'fo', 'fm', 'flush', 'fluffi', 'restock', 'florida', 'floor', 'flirt', 'resum', 'retard', 'forgeri', 'forgot', 'replenish', 'replaceeasi', 'regularli', 'frog', 'reheat', 'relat', 'releas', 'relief']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #13\n",
      "['abhor', 'needshandsfre', 'near', 'navig', 'narr', 'nargil', 'nano', 'nan', 'name', 'naan', 'mute', 'muststop', 'mussel', 'musicinclud', 'musician', 'neglig', 'murder', 'multipl', 'multigrain', 'muffl', 'muffin', 'muddl', 'mozzarella', 'movement', 'motor', 'mortifi', 'morn', 'morgan', 'monster', 'monkey']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #14\n",
      "['abhor', 'monkey', 'momentum', 'mojito', 'mode', 'mmmm', 'mix', 'misplac', 'mislead', 'mishima', 'mirrormask', 'mirag', 'minutesmajor', 'minor', 'miniusb', 'mine', 'milkshak', 'milk', 'militari', 'mile', 'middl', 'microsoft', 'michael', 'mgm', 'meth', 'metal', 'mesquit', 'merit', 'monster', 'mere']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #15\n",
      "['abhor', 'reset', 'fli', 'respect', 'flawless', 'restaraunt', 'restock', 'flatlin', 'resum', 'research', 'retard', 'fix', 'reversestereotyp', 'revisit', 'firstperson', 'ri', 'firehous', 'rib', 'ribey', 'flag', 'rick', 'flirt', 'florida', 'forth', 'fort', 'remov', 'forgot', 'forgeri', 'ford', 'fool']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #16\n",
      "['zombiez', 'subway', 'freez', 'freeway', 'freeman', 'tummi', 'subvert', 'plant', 'francisco', 'franci', 'frenchman', 'tune', 'foxx', 'fourth', 'subtl', 'forward', 'forti', 'forth', 'fort', 'turkey', 'forgot', 'fraction', 'frequent', 'tryth', 'trust', 'gadget', 'ga', 'succul', 'fuzzi', 'futur']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #17\n",
      "['abhor', 'crawfish', 'crawl', 'nostalgia', 'shooter', 'shoot', 'shoe', 'shock', 'creat', 'crema', 'crepe', 'shirt', 'crave', 'grate', 'shipment', 'shini', 'cross', 'shift', 'grandmoth', 'crumbi', 'noteworthi', 'crusti', 'shawarrrrrrma', 'crêpe', 'cuisin', 'crispi', 'cult', 'gratuiti', 'noodl', 'skimp']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #18\n",
      "['abhor', 'farc', 'relleno', 'famou', 'remak', 'fals', 'falafel', 'fair', 'releas', 'remov', 'repair', 'extraordinari', 'repeat', 'extens', 'replacementr', 'exquisit', 'express', 'factor', 'explor', 'fat', 'reheat', 'reduct', 'file', 'figur', 'reenact', 'fiancé', 'refrain', 'femal', 'relat', 'fellow']\n",
      "\n",
      "\n",
      "THE TOP 30 WORDS FOR TOPIC #19\n",
      "['abhor', 'mix', 'misplac', 'mislead', 'mishima', 'mirag', 'minutesmajor', 'minor', 'miniusb', 'mine', 'milkshak', 'milk', 'militari', 'mmmm', 'mile', 'michael', 'mgm', 'meth', 'metal', 'mesquit', 'mere', 'melvil', 'mein', 'meh', 'megapixel', 'mega', 'meeveryth', 'microsoft', 'meet', 'mojito']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the most common words by topic\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 30 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[:30]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.645607</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.591814</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.651678</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.759419</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.679350</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.018652  0.018652  0.018652  0.018652  0.018652  0.018652  0.018652   \n",
       "1  0.021483  0.021483  0.021483  0.591814  0.021483  0.021483  0.021483   \n",
       "2  0.018333  0.018333  0.018333  0.018333  0.018333  0.018333  0.018333   \n",
       "3  0.012662  0.012662  0.012662  0.012662  0.012662  0.012662  0.759419   \n",
       "4  0.016876  0.016876  0.016876  0.016876  0.016876  0.016876  0.016876   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.018652  0.018652  0.018652  0.018652  0.018652  0.645607  0.018652   \n",
       "1  0.021483  0.021483  0.021483  0.021483  0.021483  0.021483  0.021484   \n",
       "2  0.018333  0.018333  0.018333  0.018333  0.018333  0.018333  0.018333   \n",
       "3  0.012662  0.012662  0.012662  0.012662  0.012662  0.012662  0.012662   \n",
       "4  0.016876  0.016876  0.016876  0.016876  0.016876  0.016876  0.679350   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.018652  0.018652  0.018652  0.018652  0.018652  0.018652  \n",
       "1  0.021483  0.021483  0.021483  0.021483  0.021483  0.021483  \n",
       "2  0.018333  0.651678  0.018333  0.018333  0.018333  0.018333  \n",
       "3  0.012662  0.012662  0.012662  0.012662  0.012662  0.012662  \n",
       "4  0.016876  0.016876  0.016876  0.016876  0.016876  0.016876  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the model to dataset\n",
    "topic_results = LDA.transform(doc)\n",
    "data_topics = pd.DataFrame(topic_results)\n",
    "data_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2381, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.645607</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.591814</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.651678</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.759419</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.679350</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.018652  0.018652  0.018652  0.018652  0.018652  0.018652  0.018652   \n",
       "1  0.021483  0.021483  0.021483  0.591814  0.021483  0.021483  0.021483   \n",
       "2  0.018333  0.018333  0.018333  0.018333  0.018333  0.018333  0.018333   \n",
       "3  0.012662  0.012662  0.012662  0.012662  0.012662  0.012662  0.759419   \n",
       "4  0.016876  0.016876  0.016876  0.016876  0.016876  0.016876  0.016876   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0  0.018652  0.018652  0.018652  ...  0.018652  0.645607  0.018652  0.018652   \n",
       "1  0.021483  0.021483  0.021483  ...  0.021483  0.021483  0.021484  0.021483   \n",
       "2  0.018333  0.018333  0.018333  ...  0.018333  0.018333  0.018333  0.018333   \n",
       "3  0.012662  0.012662  0.012662  ...  0.012662  0.012662  0.012662  0.012662   \n",
       "4  0.016876  0.016876  0.016876  ...  0.016876  0.016876  0.679350  0.016876   \n",
       "\n",
       "         15        16        17        18        19  Topic  \n",
       "0  0.018652  0.018652  0.018652  0.018652  0.018652     12  \n",
       "1  0.021483  0.021483  0.021483  0.021483  0.021483      3  \n",
       "2  0.651678  0.018333  0.018333  0.018333  0.018333     15  \n",
       "3  0.012662  0.012662  0.012662  0.012662  0.012662      6  \n",
       "4  0.016876  0.016876  0.016876  0.016876  0.016876     13  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_topics['Topic'] = topic_results.argmax(axis=1)\n",
    "data_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_topics['Topic'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_topics = data_topics.drop('Topic', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01403044 0.01423046 0.0146556  ... 0.01441528 0.01404425 0.01406754]\n",
      " [0.017519   0.01776876 0.0182996  ... 0.01799953 0.01753624 0.01756533]\n",
      " [0.01363681 0.01383118 0.01424438 ... 0.01401081 0.01365019 0.01367283]\n",
      " ...\n",
      " [0.00599481 0.00608028 0.00626193 ... 0.00615925 0.00600071 0.00601067]\n",
      " [0.00604762 0.00613384 0.00631709 ... 0.0062135  0.00605357 0.00606361]\n",
      " [0.00753355 0.00764095 0.00786923 ... 0.00774019 0.00754097 0.00755347]]\n"
     ]
    }
   ],
   "source": [
    "# #Scaling new features\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(data_topics)\n",
    "# StandardScaler()\n",
    "# print(scaler.transform(data_topics))\n",
    "\n",
    "#Scaling new features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_topics)\n",
    "StandardScaler()\n",
    "print(scaler.transform(data_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame(data_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2381 entries, 0 to 2380\n",
      "Data columns (total 20 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       2381 non-null   float64\n",
      " 1   1       2381 non-null   float64\n",
      " 2   2       2381 non-null   float64\n",
      " 3   3       2381 non-null   float64\n",
      " 4   4       2381 non-null   float64\n",
      " 5   5       2381 non-null   float64\n",
      " 6   6       2381 non-null   float64\n",
      " 7   7       2381 non-null   float64\n",
      " 8   8       2381 non-null   float64\n",
      " 9   9       2381 non-null   float64\n",
      " 10  10      2381 non-null   float64\n",
      " 11  11      2381 non-null   float64\n",
      " 12  12      2381 non-null   float64\n",
      " 13  13      2381 non-null   float64\n",
      " 14  14      2381 non-null   float64\n",
      " 15  15      2381 non-null   float64\n",
      " 16  16      2381 non-null   float64\n",
      " 17  17      2381 non-null   float64\n",
      " 18  18      2381 non-null   float64\n",
      " 19  19      2381 non-null   float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 372.2 KB\n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 2381 entries, 0 to 2399\n",
      "Series name: Polarity\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "2381 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 37.2 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 0 to 599\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  600 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 9.4+ KB\n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 600 entries, 0 to 599\n",
      "Series name: Polarity\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "600 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 9.4 KB\n"
     ]
    }
   ],
   "source": [
    "train_x.info()\n",
    "train_y.info()\n",
    "\n",
    "test_x.info()\n",
    "test_y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying all transformations to test data set\n",
    "test_x = test_x.replace('\\d+', '', regex=True)\n",
    "test_x = test_x.replace('[^\\w\\s]','')\n",
    "text_test = list(test_x['Sentence'])\n",
    "type(text_test), len(text_test)\n",
    "\n",
    "corpus_test = []\n",
    "\n",
    "for i in range(len(text_test)):\n",
    "    r = re.sub('[^\\w\\s]','', text_test[i])\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text_test[i])\n",
    "    r = re.sub('[0-9\\n]', ' ', text_test[i])\n",
    "    r = re.sub('[\\d+]', ' ', text_test[i])\n",
    "    r = r.lower()\n",
    "    r = r.split()\n",
    "    r = [word for word in r if word not in stopwords.words('english')]\n",
    "    r = [stemmer.stem(word) for word in r]\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "    r = ' '.join(r)\n",
    "    corpus_test.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<600x2114 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4799 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_test = tfidf.fit_transform(corpus_test)\n",
    "doc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.745239</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.747587</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.013285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.738543</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.013761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.734257</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.706434</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.015451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.013408  0.013408  0.013408  0.013408  0.013408  0.745239  0.013408   \n",
       "1  0.013285  0.013285  0.013285  0.013285  0.013285  0.013285  0.013285   \n",
       "2  0.738543  0.013761  0.013761  0.013761  0.013761  0.013761  0.013761   \n",
       "3  0.013986  0.013986  0.013986  0.013986  0.013986  0.013986  0.013986   \n",
       "4  0.015451  0.706434  0.015451  0.015451  0.015451  0.015451  0.015451   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.013408  0.013408  0.013408  0.013408  0.013408  0.013408  0.013408   \n",
       "1  0.013285  0.013285  0.013285  0.013285  0.013285  0.013285  0.013285   \n",
       "2  0.013761  0.013761  0.013761  0.013761  0.013761  0.013761  0.013761   \n",
       "3  0.013986  0.013986  0.013986  0.013986  0.013986  0.013986  0.013986   \n",
       "4  0.015451  0.015451  0.015451  0.015451  0.015451  0.015451  0.015451   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.013408  0.013408  0.013408  0.013408  0.013408  0.013408  \n",
       "1  0.013285  0.013285  0.747587  0.013285  0.013285  0.013285  \n",
       "2  0.013761  0.013761  0.013761  0.013761  0.013761  0.013761  \n",
       "3  0.013986  0.734257  0.013986  0.013986  0.013986  0.013986  \n",
       "4  0.015451  0.015451  0.015451  0.015451  0.015451  0.015451  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.fit(doc_test)\n",
    "topic_results_test = LDA.transform(doc_test)\n",
    "test_x = pd.DataFrame(topic_results_test)\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(random_state=1)\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state = 1)\n",
    "print(rfc)\n",
    "print(rfc.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_lr = GridSearchCV(estimator = rfc, param_grid = param_grid, \n",
    "                        cv = 3, n_jobs = 1, verbose = 0, return_train_score=True)\n",
    "\n",
    "#Training and Prediction\n",
    "grid_lr.fit(train_x, train_y)\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "best_params = grid_lr.best_params_\n",
    "rfc.set_params(**best_params)\n",
    "rfc.fit(train_x, train_y)\n",
    "y_pred = rfc.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166666666666667"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "accuracy_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Given the accuracy and F1-score of your model, are you satisfied with the results, from a business point of view? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No I'm not satisfied with the accuracy of this model, as it is obviously quite low. It is advised this model not be utilized to predict polarity for future reviews. There are a number of reasons why this model may not have performed well:\n",
    "\n",
    "•\tThe dataset was not large enough: there were only ~2400 unique instances in the training dataset, while the testing set was ~600. NLP models, especially when unsupervised, requires significantly large datasets for better performance.\n",
    "\n",
    "•\tThe dataset had no other information or features aside from 'Sentence' and 'Polarity'. Information such as an overall numerical rating, source of the data, timestamp or more could add additional features that would further inform the model. Feature engineering was limited to text preprocessing and topic modelling (which is already limited to the size of the data). Again, this relates back to unsupervised models: both unlabeled and small data worked against the accuracy. \n",
    "\n",
    "•\tBeing limited to machine learning algorithms only, there are perhaps better algorithms that can reduce the dimensionality of a model such as LDA. For example, TextBlob or VADER are tools that, if explored, could have better predicted polarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong. Don’t just guess: dig deep to figure out the root cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "my_matrix = metrics.confusion_matrix(test_y, grid_predictions['Predicted_Label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
